{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68164d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edffff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f8388f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_LSTM import VisualCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa0063d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_rays shape: (73763, 5, 107)\n",
      "Y_pos shape: (73763, 5, 2)\n",
      "Y_vel shape: (73763, 2)\n"
     ]
    }
   ],
   "source": [
    "X_rays = np.load('../data/CNN_LSTM/updated_rays.npy')  # (N, T, R)\n",
    "Y_pos =  np.load('../data/CNN_LSTM/updated_pos.npy')    # (N, T, 2)\n",
    "Y_vel =  np.load('../data/CNN_LSTM/updated_vel.npy')    # (N, 2)\n",
    "print(\"X_rays shape:\", X_rays.shape)\n",
    "print(\"Y_pos shape:\", Y_pos.shape)\n",
    "print(\"Y_vel shape:\", Y_vel.shape)\n",
    "\n",
    "Y_pos_final = Y_pos[:, -1, :]  # position at last time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6b745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.2699\n",
      "Epoch 1: Loss = 0.1969\n",
      "Epoch 2: Loss = 0.1776\n",
      "Epoch 3: Loss = 0.1694\n",
      "Epoch 4: Loss = 0.1693\n",
      "Epoch 5: Loss = 0.1615\n",
      "Epoch 6: Loss = 0.1617\n",
      "Epoch 7: Loss = 0.1514\n",
      "Epoch 8: Loss = 0.1526\n",
      "Epoch 9: Loss = 0.1486\n",
      "Epoch 10: Loss = 0.1456\n",
      "Epoch 11: Loss = 0.1430\n",
      "Epoch 12: Loss = 0.1422\n",
      "Epoch 13: Loss = 0.1397\n",
      "Epoch 14: Loss = 0.1394\n",
      "Epoch 15: Loss = 0.1370\n",
      "Epoch 16: Loss = 0.1346\n",
      "Epoch 17: Loss = 0.1346\n",
      "Epoch 18: Loss = 0.1322\n",
      "Epoch 19: Loss = 0.1332\n",
      "Epoch 20: Loss = 0.1309\n",
      "Epoch 21: Loss = 0.1310\n",
      "Epoch 22: Loss = 0.1302\n",
      "Epoch 23: Loss = 0.1309\n",
      "Epoch 24: Loss = 0.1287\n",
      "Epoch 25: Loss = 0.1276\n",
      "Epoch 26: Loss = 0.1325\n",
      "Epoch 27: Loss = 0.1282\n",
      "Epoch 28: Loss = 0.1274\n",
      "Epoch 29: Loss = 0.1275\n",
      "Epoch 30: Loss = 0.1286\n",
      "Epoch 31: Loss = 0.1264\n",
      "Epoch 32: Loss = 0.1278\n",
      "Epoch 33: Loss = 0.1258\n",
      "Epoch 34: Loss = 0.1263\n",
      "Epoch 35: Loss = 0.1249\n",
      "Epoch 36: Loss = 0.1273\n",
      "Epoch 37: Loss = 0.1266\n",
      "Epoch 38: Loss = 0.1251\n",
      "Epoch 39: Loss = 0.1262\n",
      "Epoch 40: Loss = 0.1250\n",
      "Epoch 41: Loss = 0.1257\n",
      "Epoch 42: Loss = 0.1244\n",
      "Epoch 43: Loss = 0.1241\n",
      "Epoch 44: Loss = 0.1235\n",
      "Epoch 45: Loss = 0.1253\n",
      "Epoch 46: Loss = 0.1251\n",
      "Epoch 47: Loss = 0.1246\n",
      "Epoch 48: Loss = 0.1237\n",
      "Epoch 49: Loss = 0.1225\n"
     ]
    }
   ],
   "source": [
    "# === Flatten dataset for frame-level training ===\n",
    "# X_flat = X_rays.reshape(-1, X_rays.shape[2])           # (N*T, R)\n",
    "# Y_flat = Y_pos.reshape(-1, Y_pos.shape[2]) # (N*T, 2)\n",
    "\n",
    "X_tensor = torch.tensor(X_rays[:, -1, :], dtype=torch.float32)   # (N*T, R)\n",
    "Y_tensor = torch.tensor(Y_pos_final, dtype=torch.float32)   # (N*T, 2)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "loader = DataLoader(TensorDataset(torch.tensor(X_rays_test[:, -1, :], dtype=torch.float32), torch.tensor(Y_pos_test[:, -1,:], dtype=torch.float32)), batch_size=2, shuffle=True)\n",
    "\n",
    "# === Build model ===\n",
    "model = VisualCNN(num_rays=X_rays.shape[2])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# === Train loop ===\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        pred = model(xb)               # xb: (B, R)\n",
    "        loss = criterion(pred, yb)     # yb: (B, 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# X_tensor = torch.tensor(X_rays, dtype=torch.float32)\n",
    "# Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "# Y_target = torch.tensor(np.concatenate([Y_pos_final, Y_vel], axis=1), dtype=torch.float32)\n",
    "\n",
    "# dataset = TensorDataset(X_tensor, Y_pos_tensor, Y_target)\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# model = GatedFusionModel(num_rays=X_rays.shape[2])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# losses = []\n",
    "# for epoch in range(50):\n",
    "#     epoch_loss = 0\n",
    "#     for xb_rays, xb_pos, yb in loader:\n",
    "#         pred = model(xb_rays, xb_pos)\n",
    "#         loss = criterion(pred, yb)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#     avg_loss = epoch_loss / len(loader)\n",
    "#     losses.append(avg_loss)\n",
    "#     print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6fcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Gated Fusion Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028be09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_rays_test shape: (4701, 5, 107)\n",
      "Y_pos_test shape: (4701, 5, 2)\n",
      "Y_pos_final_test shape: (4701, 2)\n"
     ]
    }
   ],
   "source": [
    "X_rays_test = np.load('../data/CNN_LSTM_test_rays.npy')  # (N, T, R)\n",
    "Y_pos_test =  np.load('../data/CNN_LSTM_test_pos.npy')    # (N, T, 2)\n",
    "print(\"X_rays_test shape:\", X_rays_test.shape)\n",
    "print(\"Y_pos_test shape:\", Y_pos_test.shape)\n",
    "\n",
    "Y_pos_final_test = Y_pos_test[:, -1, :]  # position at last time step\n",
    "print(\"Y_pos_final_test shape:\", Y_pos_final_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97179557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\griff\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32, 2])) that is different to the input size (torch.Size([32, 0])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# pred, cnn_pos = model(xb_rays)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m loss_main \u001b[38;5;241m=\u001b[39m criterion(pred, yb)\n\u001b[1;32m---> 24\u001b[0m loss_cnn \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# position only\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_main \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.3\u001b[39m \u001b[38;5;241m*\u001b[39m loss_cnn  \u001b[38;5;66;03m# weighted training (try to make the CNN learn position)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\griff\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\griff\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\griff\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\griff\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\nn\\functional.py:3884\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[0;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3882\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3884\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n",
      "File \u001b[1;32mc:\\Users\\griff\\anaconda3\\envs\\vision\\lib\\site-packages\\torch\\functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    i = np.random.randint(0, len(X_rays_test))\n",
    "    test_ray = torch.tensor(X_rays_test[i:i+1], dtype=torch.float32)\n",
    "    test_pos = torch.tensor(Y_pos_test[i:i+1], dtype=torch.float32)\n",
    "    # print(\"Test ray shape:\", Y_pos_test[i:i+1].shape)\n",
    "    # prediction = model(test_ray, torch.zeros((1,5,2))).numpy()[0]\n",
    "    prediction = model(test_ray).numpy()[0]\n",
    "\n",
    "    print(\"Predicted (x, y, vx, vy):\", prediction)\n",
    "    print(\"Ground truth (x, y, vx, vy):\", np.concatenate([Y_pos_final_test[i], Y_vel_test[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9018fb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GatedFusionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# === Build model ===\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGatedFusionModel\u001b[49m(num_rays\u001b[38;5;241m=\u001b[39mX_rays\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m     10\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GatedFusionModel' is not defined"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X_rays, dtype=torch.float32)  # (N, T, R)\n",
    "Y_target = torch.tensor(np.concatenate([Y_pos_final], axis=1), dtype=torch.float32)  # (N, 4)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_target)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# === Build model ===\n",
    "model = GatedFusionModel(num_rays=X_rays.shape[2])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# === Train loop ===\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for xb_rays, yb in loader:\n",
    "        # pred = model(xb_rays) \n",
    "        # loss = criterion(pred, yb)\n",
    "        prediction = model(xb_rays)\n",
    "        pred = prediction[:, :4]  # position only\n",
    "        cnn_pos = prediction[:, 4:]\n",
    "        # pred, cnn_pos = model(xb_rays)\n",
    "        loss_main = criterion(pred, yb)\n",
    "        loss_cnn = criterion(cnn_pos, yb[:, :2])  # position only\n",
    "        loss = loss_main + 0.3 * loss_cnn  # weighted training (try to make the CNN learn position)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# X_tensor = torch.tensor(X_rays, dtype=torch.float32)\n",
    "# Y_pos_tensor = torch.tensor(Y_pos, dtype=torch.float32)\n",
    "# Y_target = torch.tensor(np.concatenate([Y_pos_final, Y_vel], axis=1), dtype=torch.float32)\n",
    "\n",
    "# dataset = TensorDataset(X_tensor, Y_pos_tensor, Y_target)\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# model = GatedFusionModel(num_rays=X_rays.shape[2])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# losses = []\n",
    "# for epoch in range(50):\n",
    "#     epoch_loss = 0\n",
    "#     for xb_rays, xb_pos, yb in loader:\n",
    "#         pred = model(xb_rays, xb_pos)\n",
    "#         loss = criterion(pred, yb)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#     avg_loss = epoch_loss / len(loader)\n",
    "#     losses.append(avg_loss)\n",
    "#     print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef24c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayTransformer(nn.Module):\n",
    "    def __init__(self, num_rays, d_model=64, nhead=4):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(1, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  # Output x, z\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)  # (B, R) → (B, R, 1)\n",
    "        x = self.input_proj(x)  # (B, R, d_model)\n",
    "        x = self.transformer(x)  # (B, R, d_model)\n",
    "        x = x.mean(dim=1)  # Mean pool over rays\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7743a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Flatten dataset for frame-level training ===\n",
    "# X_flat = X_rays.reshape(-1, X_rays.shape[2])           # (N*T, R)\n",
    "# Y_flat = Y_pos.reshape(-1, Y_pos.shape[2]) # (N*T, 2)\n",
    "\n",
    "X_tensor = torch.tensor(X_rays[:, -1, :], dtype=torch.float32)   # (N*T, R)\n",
    "Y_tensor = torch.tensor(Y_pos_final, dtype=torch.float32)   # (N*T, 2)\n",
    "\n",
    "ray_mean = X_tensor.mean()\n",
    "ray_std = X_tensor.std()\n",
    "X_tensor = (X_tensor - ray_mean) / ray_std\n",
    "\n",
    "target_mean = Y_tensor.mean(0)\n",
    "target_std = Y_tensor.std(0)\n",
    "Y_norm = (Y_tensor - target_mean) / target_std\n",
    "\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# === Build model ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RayTransformer(num_rays=107).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# === Train loop ===\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6bd9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
